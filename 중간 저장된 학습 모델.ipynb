{"cells":[{"cell_type":"markdown","metadata":{"id":"jG7ZEc_982io"},"source":["# StyleGAN2-ADA-PyTorch\n","\n","**Notes**\n","* Training and Inference sections should be fairly stable. I’ll slowly add new features but it should work for most mainstream use cases.\n","* Advanced Features are being documented toward the bottom of this notebook\n","\n","---\n","\n","If you find this notebook useful, consider signing up for my [Patreon](https://www.patreon.com/bustbright) or [YouTube channel](https://www.youtube.com/channel/UCaZuPdmZ380SFUMKHVsv_AA/join). You can also send me a one-time payment on [Venmo](https://venmo.com/Derrick-Schultz)."]},{"cell_type":"markdown","metadata":{"id":"Vj4PG4_i9Alt"},"source":["## Setup"]},{"cell_type":"markdown","metadata":{"id":"qGEXPcFJ9UTY"},"source":["Let’s start by checking to see what GPU we’ve been assigned. Ideally we get a V100, but a P100 is fine too. Other GPUs may lead to issues."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1661821741212,"user":{"displayName":"sida ai","userId":"14476119424192617848"},"user_tz":-540},"id":"7VVICTCvd4mc","outputId":"55dbcc12-69e4-40ad-c8e0-0600ca3d2bfb"},"outputs":[{"output_type":"stream","name":"stdout","text":["GPU 0: Tesla V100-SXM2-16GB (UUID: GPU-ed530761-0e7b-25ad-e399-82ec684350ba)\n"]}],"source":["!nvidia-smi -L"]},{"cell_type":"markdown","metadata":{"id":"rSV_HEoD9dxo"},"source":["Next let’s connect our Google Drive account. This is optional but highly recommended."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20577,"status":"ok","timestamp":1662012881169,"user":{"displayName":"sida ai","userId":"14476119424192617848"},"user_tz":-540},"id":"IuVPuJmbigRs","outputId":"ddf3ca03-5013-41aa-ec2f-f1bb964c5cce"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"nTjVmfSK9CYa"},"source":["## Install repo\n","\n","The next cell will install the StyleGAN repository in Google Drive. If you have already installed it it will just move into that folder. If you don’t have Google Drive connected it will just install the necessary code in Colab."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B8ADVNpBh8Ox"},"outputs":[],"source":["import os\n","!pip install gdown --upgrade\n","\n","if os.path.isdir(\"/content/drive/MyDrive/colab-sg2-ada-pytorch\"):\n","    %cd \"/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch\"\n","elif os.path.isdir(\"/content/drive/\"):\n","    #install script\n","    %cd \"/content/drive/MyDrive/\"\n","    !mkdir colab-sg2-ada-pytorch\n","    %cd colab-sg2-ada-pytorch\n","    !git clone https://github.com/dvschultz/stylegan2-ada-pytorch\n","    %cd stylegan2-ada-pytorch\n","    !mkdir downloads\n","    !mkdir datasets\n","    !mkdir pretrained\n","    !gdown --id 1-5xZkD8ajXw1DdopTkH_rAoCsD72LhKU -O /content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/pretrained/wikiart.pkl\n","else:\n","    !git clone https://github.com/dvschultz/stylegan2-ada-pytorch\n","    %cd stylegan2-ada-pytorch\n","    !mkdir downloads\n","    !mkdir datasets\n","    !mkdir pretrained\n","    %cd pretrained\n","    !gdown --id 1-5xZkD8ajXw1DdopTkH_rAoCsD72LhKU\n","    %cd ../"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jBeDEGqEbmLy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661821969622,"user_tz":-540,"elapsed":205212,"user":{"displayName":"sida ai","userId":"14476119424192617848"}},"outputId":"71f6c050-3b45-4e5a-c360-fe72d24ff1c5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: jax 0.3.14\n","Uninstalling jax-0.3.14:\n","  Successfully uninstalled jax-0.3.14\n","Found existing installation: jaxlib 0.3.14+cuda11.cudnn805\n","Uninstalling jaxlib-0.3.14+cuda11.cudnn805:\n","  Successfully uninstalled jaxlib-0.3.14+cuda11.cudnn805\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Looking in links: https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n","Collecting jax[cuda11_cudnn805]==0.3.10\n","  Downloading jax-0.3.10.tar.gz (939 kB)\n","\u001b[K     |████████████████████████████████| 939 kB 15.4 MB/s \n","\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from jax[cuda11_cudnn805]==0.3.10) (1.2.0)\n","Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.7/dist-packages (from jax[cuda11_cudnn805]==0.3.10) (1.21.6)\n","Requirement already satisfied: opt_einsum in /usr/local/lib/python3.7/dist-packages (from jax[cuda11_cudnn805]==0.3.10) (3.3.0)\n","Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from jax[cuda11_cudnn805]==0.3.10) (1.7.3)\n","Requirement already satisfied: typing_extensions in /usr/local/lib/python3.7/dist-packages (from jax[cuda11_cudnn805]==0.3.10) (4.1.1)\n","Collecting jaxlib==0.3.10+cuda11.cudnn805\n","  Downloading https://storage.googleapis.com/jax-releases/cuda11/jaxlib-0.3.10%2Bcuda11.cudnn805-cp37-none-manylinux2014_x86_64.whl (175.7 MB)\n","\u001b[K     |████████████████████████████████| 175.7 MB 5.4 kB/s \n","\u001b[?25hRequirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from jaxlib==0.3.10+cuda11.cudnn805->jax[cuda11_cudnn805]==0.3.10) (2.0)\n","Building wheels for collected packages: jax\n","  Building wheel for jax (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for jax: filename=jax-0.3.10-py3-none-any.whl size=1088066 sha256=ea7cfcf43c83f17fbd8811fe65f26e0bd08de4fe609aa6e296608ad315776d3e\n","  Stored in directory: /root/.cache/pip/wheels/1c/05/23/36730377cd7311156f1e5eb5e7c683d5cdac1654658c095cc5\n","Successfully built jax\n","Installing collected packages: jaxlib, jax\n","Successfully installed jax-0.3.10 jaxlib-0.3.10+cuda11.cudnn805\n","Found existing installation: torch 1.12.1+cu113\n","Uninstalling torch-1.12.1+cu113:\n","  Successfully uninstalled torch-1.12.1+cu113\n","Found existing installation: torchvision 0.13.1+cu113\n","Uninstalling torchvision-0.13.1+cu113:\n","  Successfully uninstalled torchvision-0.13.1+cu113\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Looking in links: https://download.pytorch.org/whl/torch_stable.html\n","Collecting torch==1.9.0+cu111\n","  Downloading https://download.pytorch.org/whl/cu111/torch-1.9.0%2Bcu111-cp37-cp37m-linux_x86_64.whl (2041.3 MB)\n","\u001b[K     |█████████████                   | 834.1 MB 1.2 MB/s eta 0:16:08tcmalloc: large alloc 1147494400 bytes == 0x6620c000 @  0x7fd8bbf3a615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n","\u001b[K     |████████████████▌               | 1055.7 MB 1.2 MB/s eta 0:13:20tcmalloc: large alloc 1434370048 bytes == 0x3a34000 @  0x7fd8bbf3a615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n","\u001b[K     |█████████████████████           | 1336.2 MB 1.2 MB/s eta 0:10:05tcmalloc: large alloc 1792966656 bytes == 0x59220000 @  0x7fd8bbf3a615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n","\u001b[K     |██████████████████████████▌     | 1691.1 MB 122.9 MB/s eta 0:00:03tcmalloc: large alloc 2241208320 bytes == 0xc4008000 @  0x7fd8bbf3a615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n","\u001b[K     |████████████████████████████████| 2041.3 MB 1.1 MB/s eta 0:00:01tcmalloc: large alloc 2041348096 bytes == 0x3a34000 @  0x7fd8bbf391e7 0x4a3940 0x4a39cc 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x593dd7 0x511e2c 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x548ae9\n","tcmalloc: large alloc 2551685120 bytes == 0x14996a000 @  0x7fd8bbf3a615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x593dd7 0x511e2c 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576\n","\u001b[K     |████████████████████████████████| 2041.3 MB 7.2 kB/s \n","\u001b[?25hCollecting torchvision==0.10.0+cu111\n","  Downloading https://download.pytorch.org/whl/cu111/torchvision-0.10.0%2Bcu111-cp37-cp37m-linux_x86_64.whl (23.2 MB)\n","\u001b[K     |████████████████████████████████| 23.2 MB 1.2 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0+cu111) (4.1.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==0.10.0+cu111) (1.21.6)\n","Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.10.0+cu111) (7.1.2)\n","Installing collected packages: torch, torchvision\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.9.0+cu111 which is incompatible.\n","torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.9.0+cu111 which is incompatible.\u001b[0m\n","Successfully installed torch-1.9.0+cu111 torchvision-0.10.0+cu111\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting timm==0.4.12\n","  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n","\u001b[K     |████████████████████████████████| 376 kB 15.3 MB/s \n","\u001b[?25hCollecting ftfy==6.1.1\n","  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n","\u001b[K     |████████████████████████████████| 53 kB 2.1 MB/s \n","\u001b[?25hCollecting ninja==1.10.2\n","  Downloading ninja-1.10.2-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (108 kB)\n","\u001b[K     |████████████████████████████████| 108 kB 91.4 MB/s \n","\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm==0.4.12) (0.10.0+cu111)\n","Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm==0.4.12) (1.9.0+cu111)\n","Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy==6.1.1) (0.2.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm==0.4.12) (4.1.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm==0.4.12) (1.21.6)\n","Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm==0.4.12) (7.1.2)\n","Installing collected packages: timm, ninja, ftfy\n","Successfully installed ftfy-6.1.1 ninja-1.10.2 timm-0.4.12\n"]}],"source":["#Uninstall new JAX\n","!pip uninstall jax jaxlib -y\n","#GPU frontend\n","!pip install \"jax[cuda11_cudnn805]==0.3.10\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n","#CPU frontend\n","#!pip install jax[cpu]==0.3.10\n","#Downgrade Pytorch\n","!pip uninstall torch torchvision -y\n","!pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n","!pip install timm==0.4.12 ftfy==6.1.1 ninja==1.10.2"]},{"cell_type":"markdown","metadata":{"id":"9jMmUpn4DWRe"},"source":["You probably don’t need to run this, but this will update your repo to the latest and greatest."]},{"cell_type":"code","source":["!pip install opensimplex"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tWkSP0qMafal","executionInfo":{"status":"ok","timestamp":1661823755688,"user_tz":-540,"elapsed":3318,"user":{"displayName":"sida ai","userId":"14476119424192617848"}},"outputId":"85632906-d1da-47ad-a7f8-24978c17cb8b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting opensimplex\n","  Downloading opensimplex-0.4.2-py3-none-any.whl (17 kB)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from opensimplex) (1.21.6)\n","Installing collected packages: opensimplex\n","Successfully installed opensimplex-0.4.2\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uV9bdvzeDRPd"},"outputs":[],"source":["# %cd \"/content/drive/My Drive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch\"\n","# !git config --global user.name \"test\"\n","# !git config --global user.email \"test@test.com\"\n","# !git fetch origin\n","# !git pull\n","# !git stash\n","# !git checkout origin/main -- train.py generate.py legacy.py closed_form_factorization.py flesh_digression.py apply_factor.py README.md calc_metrics.py training/stylegan2_multi.py training/training_loop.py util/utilgan.py"]},{"cell_type":"markdown","metadata":{"id":"cZkcJ58P97Ls"},"source":["## Dataset Preparation\n","\n","Upload a .zip of square images to the `datasets` folder. Previously you had to convert your model to .tfrecords. That’s no longer needed :)"]},{"cell_type":"markdown","metadata":{"id":"5B-h6FpB9FaK"},"source":["## Train model"]},{"cell_type":"markdown","metadata":{"id":"bNc-3wTO-MUd"},"source":["Below are a series of variables you need to set to run the training. You probably won’t need to touch most of them.\n","\n","* `dataset_path`: this is the path to your .zip file\n","* `resume_from`: if you’re starting a new dataset I recommend `'ffhq1024'` or `'./pretrained/wikiart.pkl'`\n","* `mirror_x` and `mirror_y`: Allow the dataset to use horizontal or vertical mirroring."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JV0W6yxP-UIn"},"outputs":[],"source":["#required: definitely edit these!\n","dataset_path = '/content/drive/MyDrive/Colab\\ Notebooks/output_images/train_set.zip'\n","resume_from = '/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/results/00004-train_set-mirror-auto1-gamma50-kimg1000-bg-resumecustom/network-snapshot-000924.pkl'\n","aug_strength = 3.425\n","train_count = 924\n","mirror_x = True\n","#mirror_y = False\n","\n","#optional: you might not need to edit these\n","gamma_value = 50.0\n","augs = 'bg'\n","config = '11gb-gpu'\n","snapshot_count = 4"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18412,"status":"ok","timestamp":1662012899577,"user":{"displayName":"sida ai","userId":"14476119424192617848"},"user_tz":-540},"id":"TD1E9CXtO3Xa","outputId":"3ee251e5-3dfd-4476-a34d-3fce17060cef"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch\n","Usage: train.py [OPTIONS]\n","\n","  Train a GAN using the techniques described in\n","  the paper \"Training Generative Adversarial\n","  Networks with Limited Data\".\n","\n","  Examples:\n","\n","  # Train with custom dataset using 1 GPU.\n","  python train.py --outdir=~/training-runs --data=~/mydataset.zip --gpus=1\n","\n","  # Train class-conditional CIFAR-10 using 2 GPUs.\n","  python train.py --outdir=~/training-runs --data=~/datasets/cifar10.zip \\\n","      --gpus=2 --cfg=cifar --cond=1\n","\n","  # Transfer learn MetFaces from FFHQ using 4 GPUs.\n","  python train.py --outdir=~/training-runs --data=~/datasets/metfaces.zip \\\n","      --gpus=4 --cfg=paper1024 --mirror=1 --resume=ffhq1024 --snap=10\n","\n","  # Reproduce original StyleGAN2 config F.\n","  python train.py --outdir=~/training-runs --data=~/datasets/ffhq.zip \\\n","      --gpus=8 --cfg=stylegan2 --mirror=1 --aug=noaug\n","\n","  Base configs (--cfg):\n","    auto       Automatically select reasonable defaults based on resolution\n","               and GPU count. Good starting point for new datasets.\n","    stylegan2  Reproduce results for StyleGAN2 config F at 1024x1024.\n","    paper256   Reproduce results for FFHQ and LSUN Cat at 256x256.\n","    paper512   Reproduce results for BreCaHAD and AFHQ at 512x512.\n","    paper1024  Reproduce results for MetFaces at 1024x1024.\n","    cifar      Reproduce results for CIFAR-10 at 32x32.\n","\n","  Transfer learning source networks (--resume):\n","    ffhq256        FFHQ trained at 256x256 resolution.\n","    ffhq512        FFHQ trained at 512x512 resolution.\n","    ffhq1024       FFHQ trained at 1024x1024 resolution.\n","    celebahq256    CelebA-HQ trained at 256x256 resolution.\n","    lsundog256     LSUN Dog trained at 256x256 resolution.\n","    <PATH or URL>  Custom network pickle.\n","\n","Options:\n","  --outdir DIR                    Where to save\n","                                  the results\n","                                  [required]\n","\n","  --gpus INT                      Number of GPUs\n","                                  to use [default:\n","                                  1]\n","\n","  --snap INT                      Snapshot\n","                                  interval\n","                                  [default: 50\n","                                  ticks]\n","\n","  --metrics LIST                  Comma-separated\n","                                  list or \"none\"\n","                                  [default:\n","                                  fid50k_full]\n","\n","  --seed INT                      Random seed\n","                                  [default: 0]\n","\n","  -n, --dry-run                   Print training\n","                                  options and exit\n","\n","  --data PATH                     Training data\n","                                  (directory or\n","                                  zip)  [required]\n","\n","  --cond BOOL                     Train\n","                                  conditional\n","                                  model based on\n","                                  dataset labels\n","                                  [default: false]\n","\n","  --subset INT                    Train with only\n","                                  N images\n","                                  [default: all]\n","\n","  --mirror BOOL                   Enable dataset\n","                                  x-flips\n","                                  [default: false]\n","\n","  --mirrory BOOL                  Augment dataset\n","                                  with y-flips\n","                                  (default: false)\n","\n","  --cfg [auto|11gb-gpu|11gb-gpu-complex|24gb-gpu|24gb-gpu-complex|48gb-gpu|48gb-2gpu|stylegan2|paper256|paper512|paper1024|cifar|cifarbaseline|aydao]\n","                                  Base config\n","                                  [default: auto]\n","\n","  --lrate FLOAT                   Override\n","                                  learning rate\n","\n","  --gamma FLOAT                   Override R1\n","                                  gamma\n","\n","  --kimg INT                      Override\n","                                  training\n","                                  duration\n","\n","  --nkimg INT                     Override\n","                                  starting count\n","\n","  --batch INT                     Override batch\n","                                  size\n","\n","  --topk FLOAT                    Enable topk\n","                                  training\n","                                  [default: None]\n","\n","  --aug [noaug|ada|fixed]         Augmentation\n","                                  mode [default:\n","                                  ada]\n","\n","  --p FLOAT                       Augmentation\n","                                  probability for\n","                                  --aug=fixed\n","\n","  --target FLOAT                  ADA target value\n","                                  for --aug=ada\n","\n","  --augpipe [blit|geom|color|filter|noise|cutout|bg|bgc|bgcf|bgcfn|bgcfnc]\n","                                  Augmentation\n","                                  pipeline\n","                                  [default: bgc]\n","\n","  --initstrength FLOAT            Override ADA\n","                                  strength at\n","                                  start\n","\n","  --resume PKL                    Resume training\n","                                  [default:\n","                                  noresume]\n","\n","  --freezed INT                   Freeze-D\n","                                  [default: 0\n","                                  layers]\n","\n","  --fp32 BOOL                     Disable mixed-\n","                                  precision\n","                                  training\n","\n","  --nhwc BOOL                     Use NHWC memory\n","                                  format with FP16\n","\n","  --nobench BOOL                  Disable cuDNN\n","                                  benchmarking\n","\n","  --allow-tf32 BOOL               Allow PyTorch to\n","                                  use TF32\n","                                  internally\n","\n","  --workers INT                   Override number\n","                                  of DataLoader\n","                                  workers\n","\n","  --help                          Show this\n","                                  message and\n","                                  exit.\n"]}],"source":["%cd /content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch\n","!python train.py --help"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EL-M7WnnfMDI","executionInfo":{"status":"ok","timestamp":1661776113845,"user_tz":-540,"elapsed":7867711,"user":{"displayName":"sida ai","userId":"14476119424192617848"}},"outputId":"338d480c-8152-4df8-fbb3-deafe6e6ae48"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Training options:\n","{\n","  \"num_gpus\": 1,\n","  \"image_snapshot_ticks\": 4,\n","  \"network_snapshot_ticks\": 4,\n","  \"metrics\": [],\n","  \"random_seed\": 0,\n","  \"training_set_kwargs\": {\n","    \"class_name\": \"training.dataset.ImageFolderDataset\",\n","    \"path\": \"/content/drive/MyDrive/Colab Notebooks/output_images/train_set.zip\",\n","    \"use_labels\": false,\n","    \"max_size\": 16382,\n","    \"xflip\": true,\n","    \"resolution\": 1024\n","  },\n","  \"data_loader_kwargs\": {\n","    \"pin_memory\": true,\n","    \"num_workers\": 3,\n","    \"prefetch_factor\": 2\n","  },\n","  \"G_kwargs\": {\n","    \"class_name\": \"training.networks.Generator\",\n","    \"z_dim\": 512,\n","    \"w_dim\": 512,\n","    \"mapping_kwargs\": {\n","      \"num_layers\": 2\n","    },\n","    \"synthesis_kwargs\": {\n","      \"channel_base\": 32768,\n","      \"channel_max\": 512,\n","      \"num_fp16_res\": 4,\n","      \"conv_clamp\": 256\n","    }\n","  },\n","  \"D_kwargs\": {\n","    \"class_name\": \"training.networks.Discriminator\",\n","    \"block_kwargs\": {},\n","    \"mapping_kwargs\": {},\n","    \"epilogue_kwargs\": {\n","      \"mbstd_group_size\": 4\n","    },\n","    \"channel_base\": 32768,\n","    \"channel_max\": 512,\n","    \"num_fp16_res\": 4,\n","    \"conv_clamp\": 256\n","  },\n","  \"G_opt_kwargs\": {\n","    \"class_name\": \"torch.optim.Adam\",\n","    \"lr\": 0.002,\n","    \"betas\": [\n","      0,\n","      0.99\n","    ],\n","    \"eps\": 1e-08\n","  },\n","  \"D_opt_kwargs\": {\n","    \"class_name\": \"torch.optim.Adam\",\n","    \"lr\": 0.002,\n","    \"betas\": [\n","      0,\n","      0.99\n","    ],\n","    \"eps\": 1e-08\n","  },\n","  \"loss_kwargs\": {\n","    \"class_name\": \"training.loss.StyleGAN2Loss\",\n","    \"r1_gamma\": 50.0\n","  },\n","  \"total_kimg\": 1000,\n","  \"batch_size\": 4,\n","  \"batch_gpu\": 4,\n","  \"ema_kimg\": 1.25,\n","  \"ema_rampup\": null,\n","  \"nimg\": 924000,\n","  \"ada_target\": 0.6,\n","  \"augment_p\": 3.425,\n","  \"augment_kwargs\": {\n","    \"class_name\": \"training.augment.AugmentPipe\",\n","    \"xflip\": 1,\n","    \"rotate90\": 1,\n","    \"xint\": 1,\n","    \"scale\": 1,\n","    \"rotate\": 1,\n","    \"aniso\": 1,\n","    \"xfrac\": 1\n","  },\n","  \"resume_pkl\": \"/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/results/00004-train_set-mirror-auto1-gamma50-kimg1000-bg-resumecustom/network-snapshot-000924.pkl\",\n","  \"ada_kimg\": 100,\n","  \"run_dir\": \"./results/00005-train_set-mirror-auto1-gamma50-kimg1000-bg-resumecustom\"\n","}\n","\n","Output directory:   ./results/00005-train_set-mirror-auto1-gamma50-kimg1000-bg-resumecustom\n","Training data:      /content/drive/MyDrive/Colab Notebooks/output_images/train_set.zip\n","Training duration:  1000 kimg\n","Number of GPUs:     1\n","Number of images:   16382\n","Image resolution:   1024\n","Conditional model:  False\n","Dataset x-flips:    True\n","\n","Creating output directory...\n","Launching processes...\n","Loading training set...\n","\n","Num images:  32764\n","Image shape: [3, 1024, 1024]\n","Label shape: [0]\n","\n","Constructing networks...\n","starting G epochs:  92.4\n","Resuming from \"/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/results/00004-train_set-mirror-auto1-gamma50-kimg1000-bg-resumecustom/network-snapshot-000924.pkl\"\n","Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n","Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n","\n","Generator              Parameters  Buffers  Output shape         Datatype\n","---                    ---         ---      ---                  ---     \n","mapping.fc0            262656      -        [4, 512]             float32 \n","mapping.fc1            262656      -        [4, 512]             float32 \n","mapping                -           512      [4, 18, 512]         float32 \n","synthesis.b4.conv1     2622465     32       [4, 512, 4, 4]       float32 \n","synthesis.b4.torgb     264195      -        [4, 3, 4, 4]         float32 \n","synthesis.b4:0         8192        16       [4, 512, 4, 4]       float32 \n","synthesis.b4:1         -           -        [4, 512, 4, 4]       float32 \n","synthesis.b8.conv0     2622465     80       [4, 512, 8, 8]       float32 \n","synthesis.b8.conv1     2622465     80       [4, 512, 8, 8]       float32 \n","synthesis.b8.torgb     264195      -        [4, 3, 8, 8]         float32 \n","synthesis.b8:0         -           16       [4, 512, 8, 8]       float32 \n","synthesis.b8:1         -           -        [4, 512, 8, 8]       float32 \n","synthesis.b16.conv0    2622465     272      [4, 512, 16, 16]     float32 \n","synthesis.b16.conv1    2622465     272      [4, 512, 16, 16]     float32 \n","synthesis.b16.torgb    264195      -        [4, 3, 16, 16]       float32 \n","synthesis.b16:0        -           16       [4, 512, 16, 16]     float32 \n","synthesis.b16:1        -           -        [4, 512, 16, 16]     float32 \n","synthesis.b32.conv0    2622465     1040     [4, 512, 32, 32]     float32 \n","synthesis.b32.conv1    2622465     1040     [4, 512, 32, 32]     float32 \n","synthesis.b32.torgb    264195      -        [4, 3, 32, 32]       float32 \n","synthesis.b32:0        -           16       [4, 512, 32, 32]     float32 \n","synthesis.b32:1        -           -        [4, 512, 32, 32]     float32 \n","synthesis.b64.conv0    2622465     4112     [4, 512, 64, 64]     float32 \n","synthesis.b64.conv1    2622465     4112     [4, 512, 64, 64]     float32 \n","synthesis.b64.torgb    264195      -        [4, 3, 64, 64]       float32 \n","synthesis.b64:0        -           16       [4, 512, 64, 64]     float32 \n","synthesis.b64:1        -           -        [4, 512, 64, 64]     float32 \n","synthesis.b128.conv0   1442561     16400    [4, 256, 128, 128]   float16 \n","synthesis.b128.conv1   721409      16400    [4, 256, 128, 128]   float16 \n","synthesis.b128.torgb   132099      -        [4, 3, 128, 128]     float16 \n","synthesis.b128:0       -           16       [4, 256, 128, 128]   float16 \n","synthesis.b128:1       -           -        [4, 256, 128, 128]   float32 \n","synthesis.b256.conv0   426369      65552    [4, 128, 256, 256]   float16 \n","synthesis.b256.conv1   213249      65552    [4, 128, 256, 256]   float16 \n","synthesis.b256.torgb   66051       -        [4, 3, 256, 256]     float16 \n","synthesis.b256:0       -           16       [4, 128, 256, 256]   float16 \n","synthesis.b256:1       -           -        [4, 128, 256, 256]   float32 \n","synthesis.b512.conv0   139457      262160   [4, 64, 512, 512]    float16 \n","synthesis.b512.conv1   69761       262160   [4, 64, 512, 512]    float16 \n","synthesis.b512.torgb   33027       -        [4, 3, 512, 512]     float16 \n","synthesis.b512:0       -           16       [4, 64, 512, 512]    float16 \n","synthesis.b512:1       -           -        [4, 64, 512, 512]    float32 \n","synthesis.b1024.conv0  51297       1048592  [4, 32, 1024, 1024]  float16 \n","synthesis.b1024.conv1  25665       1048592  [4, 32, 1024, 1024]  float16 \n","synthesis.b1024.torgb  16515       -        [4, 3, 1024, 1024]   float16 \n","synthesis.b1024:0      -           16       [4, 32, 1024, 1024]  float16 \n","synthesis.b1024:1      -           -        [4, 32, 1024, 1024]  float32 \n","---                    ---         ---      ---                  ---     \n","Total                  28794124    2797104  -                    -       \n","\n","\n","Discriminator  Parameters  Buffers  Output shape         Datatype\n","---            ---         ---      ---                  ---     \n","b1024.fromrgb  128         16       [4, 32, 1024, 1024]  float16 \n","b1024.skip     2048        16       [4, 64, 512, 512]    float16 \n","b1024.conv0    9248        16       [4, 32, 1024, 1024]  float16 \n","b1024.conv1    18496       16       [4, 64, 512, 512]    float16 \n","b1024          -           16       [4, 64, 512, 512]    float16 \n","b512.skip      8192        16       [4, 128, 256, 256]   float16 \n","b512.conv0     36928       16       [4, 64, 512, 512]    float16 \n","b512.conv1     73856       16       [4, 128, 256, 256]   float16 \n","b512           -           16       [4, 128, 256, 256]   float16 \n","b256.skip      32768       16       [4, 256, 128, 128]   float16 \n","b256.conv0     147584      16       [4, 128, 256, 256]   float16 \n","b256.conv1     295168      16       [4, 256, 128, 128]   float16 \n","b256           -           16       [4, 256, 128, 128]   float16 \n","b128.skip      131072      16       [4, 512, 64, 64]     float16 \n","b128.conv0     590080      16       [4, 256, 128, 128]   float16 \n","b128.conv1     1180160     16       [4, 512, 64, 64]     float16 \n","b128           -           16       [4, 512, 64, 64]     float16 \n","b64.skip       262144      16       [4, 512, 32, 32]     float32 \n","b64.conv0      2359808     16       [4, 512, 64, 64]     float32 \n","b64.conv1      2359808     16       [4, 512, 32, 32]     float32 \n","b64            -           16       [4, 512, 32, 32]     float32 \n","b32.skip       262144      16       [4, 512, 16, 16]     float32 \n","b32.conv0      2359808     16       [4, 512, 32, 32]     float32 \n","b32.conv1      2359808     16       [4, 512, 16, 16]     float32 \n","b32            -           16       [4, 512, 16, 16]     float32 \n","b16.skip       262144      16       [4, 512, 8, 8]       float32 \n","b16.conv0      2359808     16       [4, 512, 16, 16]     float32 \n","b16.conv1      2359808     16       [4, 512, 8, 8]       float32 \n","b16            -           16       [4, 512, 8, 8]       float32 \n","b8.skip        262144      16       [4, 512, 4, 4]       float32 \n","b8.conv0       2359808     16       [4, 512, 8, 8]       float32 \n","b8.conv1       2359808     16       [4, 512, 4, 4]       float32 \n","b8             -           16       [4, 512, 4, 4]       float32 \n","b4.mbstd       -           -        [4, 513, 4, 4]       float32 \n","b4.conv        2364416     16       [4, 512, 4, 4]       float32 \n","b4.fc          4194816     -        [4, 512]             float32 \n","b4.out         513         -        [4, 1]               float32 \n","---            ---         ---      ---                  ---     \n","Total          29012513    544      -                    -       \n","\n","Setting up augmentation...\n","Distributing across 1 GPUs...\n","Setting up training phases...\n","Exporting sample images...\n","Initializing logs...\n","Training for 1000 kimg...\n","\n","tick 0     kimg 924.0    time 1m 11s       sec/tick 8.5     sec/kimg 2127.09 maintenance 62.6   cpumem 6.75   gpumem 11.32  augment 3.425\n","tick 1     kimg 928.0    time 10m 56s      sec/tick 582.6   sec/kimg 145.65  maintenance 2.8    cpumem 7.13   gpumem 8.17   augment 3.425\n","tick 2     kimg 932.0    time 20m 27s      sec/tick 570.4   sec/kimg 142.60  maintenance 0.1    cpumem 7.13   gpumem 8.06   augment 3.436\n","tick 3     kimg 936.0    time 29m 57s      sec/tick 569.9   sec/kimg 142.49  maintenance 0.1    cpumem 7.13   gpumem 8.02   augment 3.434\n","tick 4     kimg 940.0    time 39m 27s      sec/tick 569.7   sec/kimg 142.42  maintenance 0.1    cpumem 7.13   gpumem 8.26   augment 3.460\n","tick 5     kimg 944.0    time 49m 00s      sec/tick 570.0   sec/kimg 142.50  maintenance 2.8    cpumem 7.79   gpumem 8.10   augment 3.476\n","tick 6     kimg 948.0    time 58m 30s      sec/tick 570.5   sec/kimg 142.63  maintenance 0.1    cpumem 7.79   gpumem 8.02   augment 3.488\n","tick 7     kimg 952.0    time 1h 08m 00s   sec/tick 570.0   sec/kimg 142.50  maintenance 0.1    cpumem 7.79   gpumem 8.16   augment 3.510\n","tick 8     kimg 956.0    time 1h 17m 31s   sec/tick 570.6   sec/kimg 142.66  maintenance 0.1    cpumem 7.79   gpumem 8.25   augment 3.523\n","tick 9     kimg 960.0    time 1h 27m 04s   sec/tick 570.6   sec/kimg 142.64  maintenance 2.8    cpumem 7.79   gpumem 8.14   augment 3.527\n","tick 10    kimg 964.0    time 1h 36m 35s   sec/tick 570.5   sec/kimg 142.64  maintenance 0.1    cpumem 7.79   gpumem 8.42   augment 3.525\n","tick 11    kimg 968.0    time 1h 46m 05s   sec/tick 570.0   sec/kimg 142.50  maintenance 0.1    cpumem 7.80   gpumem 8.06   augment 3.539\n","tick 12    kimg 972.0    time 1h 55m 35s   sec/tick 569.8   sec/kimg 142.44  maintenance 0.1    cpumem 7.80   gpumem 8.24   augment 3.554\n","tick 13    kimg 976.0    time 2h 05m 08s   sec/tick 570.0   sec/kimg 142.49  maintenance 2.7    cpumem 7.80   gpumem 8.05   augment 3.561\n","tick 14    kimg 980.0    time 2h 14m 37s   sec/tick 569.6   sec/kimg 142.39  maintenance 0.1    cpumem 7.80   gpumem 8.07   augment 3.575\n","tick 15    kimg 984.0    time 2h 24m 06s   sec/tick 569.1   sec/kimg 142.29  maintenance 0.1    cpumem 7.80   gpumem 8.20   augment 3.559\n","tick 16    kimg 988.0    time 2h 33m 36s   sec/tick 569.6   sec/kimg 142.39  maintenance 0.1    cpumem 7.80   gpumem 8.11   augment 3.572\n","tick 17    kimg 992.0    time 2h 43m 08s   sec/tick 569.3   sec/kimg 142.33  maintenance 2.8    cpumem 7.80   gpumem 8.32   augment 3.600\n","tick 18    kimg 996.0    time 2h 52m 38s   sec/tick 569.5   sec/kimg 142.38  maintenance 0.1    cpumem 7.80   gpumem 8.10   augment 3.623\n","tick 19    kimg 1000.0   time 3h 02m 06s   sec/tick 568.3   sec/kimg 142.22  maintenance 0.1    cpumem 7.80   gpumem 7.97   augment 3.636\n","\n","Exiting...\n"]}],"source":["!python train.py --gpus=1 --cfg=auto --metrics=None --outdir=./results --kimg=1000 --data=$dataset_path --snap=$snapshot_count --resume=$resume_from --augpipe=$augs --initstrength=$aug_strength --gamma=$gamma_value --mirror=$mirror_x --mirrory=False --nkimg=$train_count"]},{"cell_type":"markdown","metadata":{"id":"RgvSvfyi_R_-"},"source":["### Resume Training\n","\n","Once Colab has shutdown, you’ll need to resume your training. Reset the variables above, particularly the `resume_from` and `aug_strength` settings.\n","\n","1. Point `resume_from` to the last .pkl you trained (you’ll find these in the `results` folder)\n","2. Update `aug_strength` to match the augment value of the last pkl file. Often you’ll see this in the console, but you may need to look at the `log.txt`. Updating this makes sure training stays as stable as possible.\n","3. You may want to update `train_count` to keep track of your training progress.\n","\n","Once all of this has been reset, run that variable cell and the training command cell after it."]},{"cell_type":"markdown","metadata":{"id":"VznRirOE5ENI"},"source":["## Convert Legacy Model\n","\n","If you have an older version of a model (Tensorflow based StyleGAN, or Runway downloaded .pkl file) you’ll need to convert to the newest version. If you’ve trained in this notebook you do **not** need to use this cell.\n","\n","`--source`: path to model that you want to convert\n","\n","`--dest`: path and file name to convert to."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CzkP-Rww5Np9"},"outputs":[],"source":["!python legacy.py --source=/content/drive/MyDrive/runway.pkl --dest=/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/runway.pkl"]},{"cell_type":"markdown","metadata":{"id":"L6EtrPqL9ILk"},"source":["## Testing/Inference\n","\n","Also known as \"Inference\", \"Evaluation\" or \"Testing\" the model. This is the process of usinng your trained model to generate new material, usually images or videos."]},{"cell_type":"markdown","metadata":{"id":"mYdyfH0O8In_"},"source":["### Generate Single Images\n","\n","`--network`: Make sure the `--network` argument points to your .pkl file. (My preferred method is to right click on the file in the Files pane to your left and choose `Copy Path`, then paste that into the argument after the `=` sign).\n","\n","`--seeds`: This allows you to choose random seeds from the model. Remember that our input to StyleGAN is a 512-dimensional array. These seeds will generate those 512 values. Each seed will generate a different, random array. The same seed value will also always generate the same random array, so we can later use it for other purposes like interpolation.\n","\n","`--truncation`: Truncation, well, truncates the latent space. This can have a subtle or dramatic affect on your images depending on the value you use. The smaller the number the more realistic your images should appear, but this will also affect diversity. Most people choose between 0.5 and 1.0, but technically it's infinite. \n"]},{"cell_type":"code","source":["out = '/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/out'\n","net = '/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/results/00005-train_set-mirror-auto1-gamma50-kimg1000-bg-resumecustom/network-snapshot-001000.pkl'"],"metadata":{"id":"GPPWy3YcZ9xz"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VYRXenMoZSHf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661824183927,"user_tz":-540,"elapsed":8300,"user":{"displayName":"sida ai","userId":"14476119424192617848"}},"outputId":"aecdb27c-cc8b-46b5-e680-b7c67a0c7496"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading networks from \"/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/results/00005-train_set-mirror-auto1-gamma50-kimg1000-bg-resumecustom/network-snapshot-001000.pkl\"...\n","Generating image for seed 0 (0/1) ...\n","Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n","Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n"]}],"source":["!python generate.py --outdir=out --trunc=-1 --seeds=0 --network=/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/results/00005-train_set-mirror-auto1-gamma50-kimg1000-bg-resumecustom/network-snapshot-001000.pkl"]},{"cell_type":"code","source":["!python generate.py --outdir=out --trunc=0 --seeds=1 --network=/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/results/00005-train_set-mirror-auto1-gamma50-kimg1000-bg-resumecustom/network-snapshot-001000.pkl"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ilVFL2yjbvHw","executionInfo":{"status":"ok","timestamp":1661824191860,"user_tz":-540,"elapsed":7936,"user":{"displayName":"sida ai","userId":"14476119424192617848"}},"outputId":"16114bc5-0ff4-42a4-cafa-f0578ac38e84"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading networks from \"/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/results/00005-train_set-mirror-auto1-gamma50-kimg1000-bg-resumecustom/network-snapshot-001000.pkl\"...\n","Generating image for seed 1 (0/1) ...\n","Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n","Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n"]}]},{"cell_type":"code","source":["!python generate.py --outdir=out --trunc=1 --seeds=2 --network=/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/results/00005-train_set-mirror-auto1-gamma50-kimg1000-bg-resumecustom/network-snapshot-001000.pkl"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0eeVXjOCb3IU","executionInfo":{"status":"ok","timestamp":1661824199806,"user_tz":-540,"elapsed":7973,"user":{"displayName":"sida ai","userId":"14476119424192617848"}},"outputId":"2791049d-c35e-45f6-9682-cedf068c4b70"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading networks from \"/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/results/00005-train_set-mirror-auto1-gamma50-kimg1000-bg-resumecustom/network-snapshot-001000.pkl\"...\n","Generating image for seed 2 (0/1) ...\n","Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n","Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n"]}]},{"cell_type":"markdown","metadata":{"id":"_EKihY26OmTD"},"source":["#### Non-Square outputs\n","\n","We can modify the model to output images that are not square. This isn’t as good as training a rectangular model, but with the right model it can still look nice.\n","\n","* `--size` size takes in a value of `xdim-ydim`. For example, to generate a 1920x1080 image use `1920-1080`\n","* `--scale-type` This determines the padding style to apply in the additional space. There are four options: `pad`, `padside`, `symm`, and `symmside`. I recommend trying each one to see what works best with your images.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zuJxj-gdO3G0"},"outputs":[],"source":["!python generate.py --outdir=/content/out/images/ --trunc=0.7 --size=1820-1024 --scale-type=symm --seeds=0-499 --network=/content/crystal.pkl"]},{"cell_type":"markdown","metadata":{"id":"E_sGJowuPwwE"},"source":["We can use these options for any image or video generation commands (excluding projection)."]},{"cell_type":"markdown","metadata":{"id":"VjOTCWVonoVL"},"source":["### Truncation Traversal\n","\n","Below you can take one seed and look at the changes to it across any truncation amount. -1 to 1 will be pretty realistic images, but the further out you get the weirder it gets.\n","\n","#### Options \n","`--network`: Again, this should be the path to your .pkl file.\n","\n","`--seeds`: Pass this only one seed. Pick a favorite from your generated images.\n","\n","`--start`: Starting truncation value.\n","\n","`--stop`: Stopping truncation value. This should be larger than the start value. (Will probably break if its not).\n","\n","`--increment`: How much each frame should increment the truncation value. Make this really small if you want a long, slow interpolation. (stop-start/increment=total frames)\n"]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9PphPDb2UQFo","executionInfo":{"status":"ok","timestamp":1661822120881,"user_tz":-540,"elapsed":4,"user":{"displayName":"sida ai","userId":"14476119424192617848"}},"outputId":"4d02c888-2234-43a5-f948-d79f64a63d2f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"5bkGjGOwU3c-"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nyzdGr7OnrMG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661822256983,"user_tz":-540,"elapsed":1053,"user":{"displayName":"sida ai","userId":"14476119424192617848"}},"outputId":"8a9ffa35-8351-44e2-9491-256220b80750"},"outputs":[{"output_type":"stream","name":"stdout","text":["Traceback (most recent call last):\n","  File \"generate.py\", line 25, in <module>\n","    from opensimplex import OpenSimplex\n","ModuleNotFoundError: No module named 'opensimplex'\n"]}],"source":["!python generate.py --process=\"truncation\" --outdir=/content/out/trunc-trav-3/ --start=-0.8 --stop=2.8 --increment=0.02 --seeds=470 --network=/content/drive/MyDrive/stylegan2-transfer-models//content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/results//content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/results/00005-train_set-mirror-auto1-gamma50-kimg1000-bg-resumecustom/00005-train_set-mirror-auto1-gamma50-kimg1000-bg-resumecustom/network-snapshot-001000.pkl"]},{"cell_type":"markdown","metadata":{"id":"OSzj0igO8Lfu"},"source":["### Interpolations\n","\n","Interpolation is the process of generating very small changes to a vector in order to make it appear animated from frame to frame.\n","\n","We’ll look at different examples of interpolation below.\n","\n","#### Options\n","\n","`--network`: path to your .pkl file\n","\n","`--interpolation`: Walk type defines the type of interpolation you want. In some cases it can also specify whether you want the z space or the w space.\n","\n","`--frames`: How many frames you want to produce. Use this to manage the length of your video.\n","\n","`--trunc`: truncation value"]},{"cell_type":"markdown","metadata":{"id":"OJSqafIzNwhx"},"source":["#### Linear Interpolation (선형 보간)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sqkiskly8S5_"},"outputs":[],"source":["!python generate.py --outdir=/content/out/video1-w-0.5/ --space=\"z\" --trunc=0.5 --process=\"interpolation\" --seeds=463,470 --network=/content/drive/MyDrive/stylegan2-transfer-models/mixed6k-network-snapshot-016470.pkl"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DCUEV3aO8s_X"},"outputs":[],"source":["!python generate.py --outdir=out/video1-w/ --space=\"w\" --trunc=1 --process=\"interpolation\" --seeds=85,265,297,849 --network=/content/stylegan2-ada-pytorch/pretrained/wikiart.pkl"]},{"cell_type":"markdown","metadata":{"id":"Yi3d7xzpN2Uj"},"source":["#### Slerp Interpolation (구면 선형보간)\n","\n","This gets a little heady, but technically linear interpolations are not the best in high-dimensional GANs. [This github link](https://github.com/soumith/dcgan.torch/issues/14) is one of the more popular explanations ad discussions.\n","\n","In reality I do not find a huge difference between linear and spherical interpolations (the difference in z- and w-space is enough in many cases), but I’ve implemented slerp here for anyone interested."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I0-cUd3fB_kJ"},"outputs":[],"source":["!python generate.py --outdir=out/slerp-z/ --space=\"z\" --trunc=1 --process=\"interpolation\" --interpolation=\"slerp\" --seeds=85,265,297,849 --network=/content/stylegan2-ada-pytorch/pretrained/wikiart.pkl --frames=24"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PtnBIF75pcoY"},"outputs":[],"source":["!python generate.py --outdir=out/slerp-w/ --space=\"w\" --trunc=1 --process=\"interpolation\" --interpolation=\"slerp\" --seeds=85,265,297,849 --network=/content/stylegan2-ada-pytorch/pretrained/wikiart.pkl --frames=12"]},{"cell_type":"markdown","metadata":{"id":"uP1HsU_CPcF5"},"source":["#### Noise Loop\n","\n","If you want to just make a random but fun interpolation of your model the noise loop is the way to go. It creates a random path thru the z space to show you a diverse set of images.\n","\n","`--interpolation=\"noiseloop\"`: set this to use the noise loop funtion\n","\n","`--diameter`: This controls how \"wide\" the loop is. Make it smaller to show a less diverse range of samples. Make it larger to cover a lot of samples. This plus `--frames` can help determine how fast the video feels.\n","\n","`--random_seed`: this allows you to change your starting place in the z space. Note: this value has nothing to do with the seeds you use to generate images. It just allows you to randomize your start point (and if you want to return to it you can use the same seed multiple times).\n","\n","Noise loops currently only work in z space."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gfR6DhfvN8b_"},"outputs":[],"source":["!python generate.py --outdir=out/video-noiseloop-0.9d/ --trunc=0.8 --process=\"interpolation\" --interpolation=\"noiseloop\" --diameter=0.9 --random_seed=100 --network=/content/stylegan2-ada-pytorch/pretrained/wikiart.pkl"]},{"cell_type":"markdown","metadata":{"id":"PkKFb-4CedOq"},"source":["#### Circular Loop\n","\n","The noise loop is, well, noisy. This circular loop will feel much more even, while still providing a random loop.\n","\n","I recommend using a higher `--diameter` value than you do with noise loops. Something between `50.0` and `500.0` alongside `--frames` can help control speed and diversity."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ao62za9_QfOF"},"outputs":[],"source":["!python generate.py --outdir=out/video-circularloop/ --trunc=1 --process=\"interpolation\" --interpolation=\"circularloop\" --diameter=800.00 --frames=720 --random_seed=90 --network=/content/stylegan2-ada-pytorch/pretrained/wikiart.pkl"]},{"cell_type":"markdown","metadata":{"id":"qz-fVtzyAHg1"},"source":["## Projection"]},{"cell_type":"markdown","metadata":{"id":"ez7tXSpCA_zh"},"source":["### Basic Projector\n","\n","*   `--target`: this is a path to the image file that you want to \"find\" in your model. This image must be the exact same size as your model.\n","*   `--num-steps`: how many iterations the projctor should run for. Lower will mean less steps and less likelihood of a good projection. Higher will take longer but will likely produce better images.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p84CtZUGAKnR"},"outputs":[],"source":["!python projector.py --help"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"80YTcjIQARWh"},"outputs":[],"source":["!python projector.py --network=/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/results/00023-chin-morris-mirror-11gb-gpu-gamma50-bg-resumecustom/network-snapshot-000304.pkl --outdir=/content/projector/ --target=/content/img005421_0.png --num-steps=200 --seed=0"]},{"cell_type":"markdown","metadata":{"id":"hAxADbdpHHib"},"source":["### Peter Baylies’ Projector"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iwS_ey9QF-nk"},"outputs":[],"source":["!python /content/stylegan2-ada-pytorch/pbaylies_projector.py --help"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-yj06MAABoLe"},"outputs":[],"source":["!python /content/stylegan2-ada-pytorch/pbaylies_projector.py --network=/content/ladiesblack.pkl --outdir=/content/projector-no-clip-006265-4-inv-3k/ --target-image=/content/img006265-4-inv.png --num-steps=3000 --use-clip=False --use-center=False --seed=99"]},{"cell_type":"markdown","metadata":{"id":"qywlaS5pgzyH"},"source":["## Combine NPZ files together"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M2VooqrNfIpw"},"outputs":[],"source":["!python combine_npz.py --outdir=/content/npz --npzs='/content/projector-no-clip-006264-1-inv-3k/projector-no-clip-006264-1-inv-3k.npz,/content/projector-no-clip-006265-1-inv-3k/projector-no-clip-006265-1-inv-3k.npz,/content/projector-no-clip-006264-5-inv-3k/projector-no-clip-006264-5-inv-3k.npz,/content/projector-no-clip-006265-3-inv-3k/projector-no-clip-006265-3-inv-3k.npz,/content/projector-no-clip-006265-4-inv-3k/projector-no-clip-006265-4-inv-3k.npz,/content/projector-no-clip-006264-1-inv-3k/projector-no-clip-006264-1-inv-3k.npz'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uIqgl5nIHwpp"},"outputs":[],"source":["!python generate.py --help"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4cgezYN8Dsyh"},"outputs":[],"source":["!python generate.py --process=interpolation --interpolation=linear --easing=easeInOutQuad --space=w --network=/content/ladiesblack.pkl --outdir=/content/combined-proj/ --projected-w=/content/npz/combined.npz --frames=120"]},{"cell_type":"markdown","metadata":{"id":"lF7RCnSAsWrq"},"source":["## Feature Extraction using Closed Form Factorization\n","\n","Feature Extraction is the process of finding “human readable” vectors in a StyleGAN model. For example, let’s say you wanted to find a vector that could open or close a mouth in a face model.\n","\n","The feature extractor tries to automate the procss of finding important vectors in your model.\n","\n","`--ckpt`: This is the path to your .pkl file. In other places its called `--network` (It’s a long story for why its name changed here)\n","`--out`: path to save your output feature vector file. The file name must end in `.pt`!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Hek6TFZCKD-"},"outputs":[],"source":["!python closed_form_factorization.py --out=/content/ladiesblack-cff.pt --ckpt=/content/ladiesblack.pkl"]},{"cell_type":"markdown","metadata":{"id":"WxLgeNeJRqFh"},"source":["Once this cell is finished you’ll want to save that `.pt` file somewhere for reuse.\n","\n","This process just created the vctor values, but we need to test it on some seed values to determine what each vector actually changes. The `apply_factor.py` script does this.\n","\n","Arguments to try:\n","\n","\n","*   `-i`: This stands for index. By default, the cell above will produce 512 vectors, so `-i` can be any value from 0 to 511. I recommend starting with a higher value.\n","*   `-d`: This stands for degrees. This means how much change you want to see along th vector. I recommend a value between 5 and 10 to start with.\n","*   `--seeds`: You know what these are by now right? :)\n","*   `--ckpt`: path to your .pkl file\n","*   `--video`: adding this to your argument will produce a video that animates your seeds along the vector path. I find it much easier to figure out what’s changing with an animation.\n","*   `--output`: where to save the images/video\n","*   `--space`: By default this will use the w space to reduce entanglement\n","\n","Lastly you need to add the path to the `.pt` file you made in th above cell. It’s weird, but you don’t need to add any arguments bfore it, just make sure its after `apply_factor.pt`\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dEDSl2VpCSJL"},"outputs":[],"source":["!python apply_factor.py -i 0 -d 10 --seeds 5,10 --ckpt /content/ladiesblack.pkl /content/ladiesblack-cff.pt --output /content/cff-vid/ --video"]},{"cell_type":"markdown","metadata":{"id":"mzwhrjGlTMZ3"},"source":["That just produced images or video for a single vector, but there are 511 more! To generate every vector, you can uuse the cell below. Update any arguments you want, but don’t touch the `-i {i}` part.\n","\n","**Warning:** This takes a long time, especially if you have more than one seed value (pro tip: don’t usee more than one seed value)! Also, this will take up a good amount of space in Google Drive. You’ve been warned!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6aFj6mcKDmqk"},"outputs":[],"source":["for i in range(512):\n","  !python apply_factor.py -i {i} -d 10 --seeds 177 --ckpt /content/drive/MyDrive/network-snapshot-008720.pkl /content/ladies-black-cff.pt --output /content/drive/MyDrive/ladiesblack-cff-17/ --video #--out_prefix 'ladiesblack-factor-{i}'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UVfmNV5JEcdp"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"3VLRzmilrCf4"},"source":["# Layer Manipulations\n","\n","The following scripts allow you to modify various resolution layers of the StyleGAN model."]},{"cell_type":"markdown","metadata":{"id":"BDpQrBdevrDj"},"source":["## Flesh Digressions\n","\n","Flesh Digressions works by manipulating the vectors in the base 4x4 layer. By doing this while leaving all the other layers untouched you can create a warping and twisting version of images from your model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BdvBNkMZv4MK"},"outputs":[],"source":["!python flesh_digression.py --pkl /content/stylegan2-ada-pytorch/pretrained/wikiart.pkl --psi 0.5 --seed 9999"]},{"cell_type":"markdown","metadata":{"id":"G2TrnyvprL42"},"source":["## Network Blending\n","You can take two completely different models and combine them by splitting them at a specific resolution and combining the lower layers of one model and the higher layers of another.\n","\n","(Note: this tends to work best when one of the models is transfer learned from the other)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n6pjl31Jwa4u"},"outputs":[],"source":["!python blend_models.py --lower_res_pkl /content/ffhq-pt.pkl --split_res 64 --higher_res_pkl /content/bone-bone-pt.pkl --output_path /content/ffhq-bonebone-split64.pkl"]},{"cell_type":"markdown","metadata":{"id":"futaO6lBroVH"},"source":["You can now take the output .pkl file and use that with any of the generation tools above."]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["_EKihY26OmTD","OJSqafIzNwhx","Yi3d7xzpN2Uj","uP1HsU_CPcF5","PkKFb-4CedOq"],"machine_shape":"hm","provenance":[{"file_id":"https://github.com/dvschultz/stylegan2-ada-pytorch/blob/main/SG2_ADA_PyTorch.ipynb","timestamp":1661610266622}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}