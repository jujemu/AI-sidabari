{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"mount_file_id":"1-D5ohiP4C8CeVKYTHr6fUnEaSKAdHh-R","authorship_tag":"ABX9TyMoq9W1ZYAkAsJdkCnlYkI5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["%%capture\n","!pip install cvlib"],"metadata":{"id":"7RrPH_onj4WR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd /content\n","!gdown --id 13nvlIOMwfSFCSRdO-h_YNnfpGwhCwPXa\n","!gdown --id 1u_tgBn4BR5mPjCSBxv-yBQc9y2cQwP52"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ey0KTx24jgzL","executionInfo":{"status":"ok","timestamp":1663555070929,"user_tz":-540,"elapsed":11642,"user":{"displayName":"sida ai","userId":"14476119424192617848"}},"outputId":"dacaeced-21db-4624-8f88-10f30175600c"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n","/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n","  category=FutureWarning,\n","Downloading...\n","From: https://drive.google.com/uc?id=13nvlIOMwfSFCSRdO-h_YNnfpGwhCwPXa\n","To: /content/model.h5\n","100% 182M/182M [00:03<00:00, 49.7MB/s]\n","/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n","  category=FutureWarning,\n","Downloading...\n","From: https://drive.google.com/uc?id=1u_tgBn4BR5mPjCSBxv-yBQc9y2cQwP52\n","To: /content/weights.h5\n","100% 91.0M/91.0M [00:01<00:00, 51.9MB/s]\n"]}]},{"cell_type":"code","source":["data_path = '/content/drive/MyDrive/Colab Notebooks/AiffelThon/gender_classification/'"],"metadata":{"id":"FzAaCASPnHvh","executionInfo":{"status":"ok","timestamp":1663555162552,"user_tz":-540,"elapsed":406,"user":{"displayName":"sida ai","userId":"14476119424192617848"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2QpDjqwDg2wI","executionInfo":{"status":"ok","timestamp":1663555643402,"user_tz":-540,"elapsed":1486,"user":{"displayName":"sida ai","userId":"14476119424192617848"}},"outputId":"e4f8a628-cab8-407b-ae53-69a64f40a695"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5fca3c1200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"stream","name":"stdout","text":["[0.80704564 0.19295436]\n","Male\n"]}],"source":["from keras.preprocessing.image import img_to_array\n","from keras.models import load_model\n","import numpy as np\n","import cv2\n","import os\n","import cvlib as cv\n","\n","# model path\n","model_path = \"model.h5\"\n","model_weights_path = \"weights.h5\"\n","\n","image_path = data_path + '4.png'\n","im = cv2.imread(image_path)\n","\n","# load model\n","model = load_model(model_path)\n","model.load_weights(model_weights_path)\n","\n","\n","faces, confidences = cv.detect_face(im)\n","# loop through detected faces and add bounding box\n","for face in faces:\n","    (startX,startY) = face[0],face[1]\n","    (endX,endY) = face[2],face[3]\n","    # draw rectangle over face\n","    cv2.rectangle(im, (startX,startY), (endX,endY), (0,100,0), 2)\n","\n","    # preprocessing for gender detection model\n","    cropped_face = im[startY:endY,startX:endX]\n","    cropped_face = cv2.resize(cropped_face, (150,150))\n","    cropped_face = cropped_face.astype(\"float32\") / 255\n","    cropped_face = img_to_array(cropped_face)\n","    cropped_face = np.expand_dims(cropped_face, axis=0)\n","\n","    # apply model\n","    conf = model.predict(cropped_face)[0]\n","\n","    if conf[0] > conf[1]:\n","        if conf[0] - conf[1] < conf[1]:\n","            label = \"Maybe male\"\n","        else:\n","            label = \"Male\"\n","    else:\n","        if conf[0] < conf[1]:\n","            if conf[1] - conf[0] < conf[0]:\n","                label = \"Maybe female\"\n","            else:\n","                label = \"Female\"\n","\n","    print(conf)\n","    print(label)\n","\n","    if label.find(\"Maybe\") >= 0: \n","        cv2.putText(im, label, (startX, startY-5),  cv2.FONT_HERSHEY_SIMPLEX,0.5, (200,0,0), 2)\n","    else:\n","        cv2.putText(im, label, (startX, startY-5),  cv2.FONT_HERSHEY_SIMPLEX,1, (200,0,0), 2) "]},{"cell_type":"code","source":["model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XrHhx6ASkEjd","executionInfo":{"status":"ok","timestamp":1663555740940,"user_tz":-540,"elapsed":9,"user":{"displayName":"sida ai","userId":"14476119424192617848"}},"outputId":"efbeaf12-a342-4c8a-9e6e-1ea7db47b36d"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_1 (Conv2D)           (None, 150, 150, 32)      896       \n","                                                                 \n"," activation_1 (Activation)   (None, 150, 150, 32)      0         \n","                                                                 \n"," max_pooling2d_1 (MaxPooling  (None, 75, 75, 32)       0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 75, 75, 64)        8256      \n","                                                                 \n"," activation_2 (Activation)   (None, 75, 75, 64)        0         \n","                                                                 \n"," max_pooling2d_2 (MaxPooling  (None, 75, 37, 32)       0         \n"," 2D)                                                             \n","                                                                 \n"," flatten_1 (Flatten)         (None, 88800)             0         \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               22733056  \n","                                                                 \n"," activation_3 (Activation)   (None, 256)               0         \n","                                                                 \n"," dropout_1 (Dropout)         (None, 256)               0         \n","                                                                 \n"," dense_2 (Dense)             (None, 2)                 514       \n","                                                                 \n","=================================================================\n","Total params: 22,742,722\n","Trainable params: 22,742,722\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Y_HrRWGhpglj"},"execution_count":null,"outputs":[]}]}